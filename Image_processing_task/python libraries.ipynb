{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1.\tpython libraries to deal with images**\n",
        "> 1.   openCV\n",
        "2.   Scikit-Image\n",
        "3.   SciPy\n",
        "4.   Pillow/PIL\n",
        "5.   NumPy\n",
        "6.   SimpleITK\n",
        "7.   Pgmagick\n",
        "8.   Mahotas"
      ],
      "metadata": {
        "id": "FRw51HfAKi-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.\thow to read image and sound from video using python**\n",
        "> here read the imsge from video"
      ],
      "metadata": {
        "id": "ispYWSsTNoRg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ykprVLoIiAi"
      },
      "outputs": [],
      "source": [
        "# Importing all necessary libraries\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Read the video from specified path\n",
        "cam = cv2.VideoCapture(\"C:\\\\Users\\\\Admin\\\\PycharmProjects\\\\project_1\\\\openCV.mp4\")\n",
        "\n",
        "try:\n",
        "\n",
        "\t# creating a folder named data\n",
        "\tif not os.path.exists('data'):\n",
        "\t\tos.makedirs('data')\n",
        "\n",
        "# if not created then raise error\n",
        "except OSError:\n",
        "\tprint ('Error: Creating directory of data')\n",
        "\n",
        "# frame\n",
        "currentframe = 0\n",
        "\n",
        "while(True):\n",
        "\n",
        "\t# reading from frame\n",
        "\tret,frame = cam.read()\n",
        "\n",
        "\tif ret:\n",
        "\t\t# if video is still left continue creating images\n",
        "\t\tname = './data/frame' + str(currentframe) + '.jpg'\n",
        "\t\tprint ('Creating...' + name)\n",
        "\n",
        "\t\t# writing the extracted images\n",
        "\t\tcv2.imwrite(name, frame)\n",
        "\n",
        "\t\t# increasing counter so that it will\n",
        "\t\t# show how many frames are created\n",
        "\t\tcurrentframe += 1\n",
        "\telse:\n",
        "\t\tbreak\n",
        "\n",
        "# Release all space and windows once done\n",
        "cam.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> here read the audio from video"
      ],
      "metadata": {
        "id": "qdDORfpnKhL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we get a youtube link and extract only the audio when downloading it\n",
        "import yt_dlp\n",
        "\n",
        "urls = ['https://www.youtube.com/watch?v=xpVfcZ0ZcFM', 'https://www.youtube.com/watch?v=2Vv-BfVoq4g', 'https://www.youtube.com/watch?v=zDo0H8Fm7d0', 'https://www.youtube.com/watch?v=BQ0mxQXmLsk', 'https://www.youtube.com/watch?v=d1wy6LWfHAs']\n",
        "\n",
        "for url in urls:\n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'postprocessors': [{\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'mp3',\n",
        "            'preferredquality': '192',\n",
        "        }],\n",
        "        'ffmpeg_location': 'C:/ffmpeg-2023-07-13-git-9a2335444b-essentials_build/bin/ffmpeg.exe',\n",
        "        'ffprobe_location': 'C:/ffmpeg-2023-07-13-git-9a2335444b-essentials_build/bin/ffprobe.exe'\n",
        "    }\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([url])"
      ],
      "metadata": {
        "id": "cpEVvFZCKqNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.\tcomputer vision libraries python**\n",
        "> 1.    OpenCV\n",
        "2.    TensorFlow\n",
        "3.    SimpleCV\n",
        "4.    Caffe\n",
        "5.    PyTorch\n",
        "6.    Keras\n",
        "7.    Detectorn2"
      ],
      "metadata": {
        "id": "wak9LKAfdeD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.\tface detection code**"
      ],
      "metadata": {
        "id": "9-ASB9NGeBo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization utilities\n",
        "\n",
        "> To better demonstrate the Face Detector API, we have created a set of visualization tools that will be used in this colab. These will draw a bounding box around detected faces, as well as markers over certain detected points on the faces."
      ],
      "metadata": {
        "id": "stvzzqc8eXop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple, Union\n",
        "import math\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "MARGIN = 10  # pixels\n",
        "ROW_SIZE = 10  # pixels\n",
        "FONT_SIZE = 1\n",
        "FONT_THICKNESS = 1\n",
        "TEXT_COLOR = (255, 0, 0)  # red\n",
        "\n",
        "\n",
        "def _normalized_to_pixel_coordinates(\n",
        "    normalized_x: float, normalized_y: float, image_width: int,\n",
        "    image_height: int) -> Union[None, Tuple[int, int]]:\n",
        "  \"\"\"Converts normalized value pair to pixel coordinates.\"\"\"\n",
        "\n",
        "  # Checks if the float value is between 0 and 1.\n",
        "  def is_valid_normalized_value(value: float) -> bool:\n",
        "    return (value > 0 or math.isclose(0, value)) and (value < 1 or\n",
        "                                                      math.isclose(1, value))\n",
        "\n",
        "  if not (is_valid_normalized_value(normalized_x) and\n",
        "          is_valid_normalized_value(normalized_y)):\n",
        "    # TODO: Draw coordinates even if it's outside of the image bounds.\n",
        "    return None\n",
        "  x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n",
        "  y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n",
        "  return x_px, y_px\n",
        "\n",
        "\n",
        "def visualize(\n",
        "    image,\n",
        "    detection_result\n",
        ") -> np.ndarray:\n",
        "  \"\"\"Draws bounding boxes and keypoints on the input image and return it.\n",
        "  Args:\n",
        "    image: The input RGB image.\n",
        "    detection_result: The list of all \"Detection\" entities to be visualize.\n",
        "  Returns:\n",
        "    Image with bounding boxes.\n",
        "  \"\"\"\n",
        "  annotated_image = image.copy()\n",
        "  height, width, _ = image.shape\n",
        "\n",
        "  for detection in detection_result.detections:\n",
        "    # Draw bounding_box\n",
        "    bbox = detection.bounding_box\n",
        "    start_point = bbox.origin_x, bbox.origin_y\n",
        "    end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
        "    cv2.rectangle(annotated_image, start_point, end_point, TEXT_COLOR, 3)\n",
        "\n",
        "    # Draw keypoints\n",
        "    for keypoint in detection.keypoints:\n",
        "      keypoint_px = _normalized_to_pixel_coordinates(keypoint.x, keypoint.y,\n",
        "                                                     width, height)\n",
        "      color, thickness, radius = (0, 255, 0), 2, 2\n",
        "      cv2.circle(annotated_image, keypoint_px, thickness, color, radius)\n",
        "\n",
        "    # Draw label and score\n",
        "    category = detection.categories[0]\n",
        "    category_name = category.category_name\n",
        "    category_name = '' if category_name is None else category_name\n",
        "    probability = round(category.score, 2)\n",
        "    result_text = category_name + ' (' + str(probability) + ')'\n",
        "    text_location = (MARGIN + bbox.origin_x,\n",
        "                     MARGIN + ROW_SIZE + bbox.origin_y)\n",
        "    cv2.putText(annotated_image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
        "                FONT_SIZE, TEXT_COLOR, FONT_THICKNESS)\n",
        "\n",
        "  return annotated_image"
      ],
      "metadata": {
        "id": "bWoukcFVd99s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download test image\n",
        "\n",
        "> To demonstrate Face Detection, you can download a sample image using the following code. Credits: https://pixabay.com/photos/brother-sister-girl-family-boy-977170/"
      ],
      "metadata": {
        "id": "ee5A7hIiei9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O image.jpg https://i.imgur.com/Vu2Nqwb.jpg\n",
        "\n",
        "IMAGE_FILE = 'image.jpg'\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "img = cv2.imread(IMAGE_FILE)\n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "id": "jgprcAUFeOqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running inference and visualizing the results\n",
        "\n",
        "> The final step is to run face detection on your selected image. This involves creating your FaceDetector object, loading your image, running detection, and finally, the optional step of displaying the image with visualizations.\n",
        "\n",
        "> You can check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/face_detector/python) to learn more about configuration options that this solution supports."
      ],
      "metadata": {
        "id": "_4HKcMqQfG5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Import the necessary modules.\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "\n",
        "# STEP 2: Create an FaceDetector object.\n",
        "base_options = python.BaseOptions(model_asset_path='detector.tflite')\n",
        "options = vision.FaceDetectorOptions(base_options=base_options)\n",
        "detector = vision.FaceDetector.create_from_options(options)\n",
        "\n",
        "# STEP 3: Load the input image.\n",
        "image = mp.Image.create_from_file(IMAGE_FILE)\n",
        "\n",
        "# STEP 4: Detect faces in the input image.\n",
        "detection_result = detector.detect(image)\n",
        "\n",
        "# STEP 5: Process the detection result. In this case, visualize it.\n",
        "image_copy = np.copy(image.numpy_view())\n",
        "annotated_image = visualize(image_copy, detection_result)\n",
        "rgb_annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
        "cv2_imshow(rgb_annotated_image)"
      ],
      "metadata": {
        "id": "lKIo6pAReOnZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}